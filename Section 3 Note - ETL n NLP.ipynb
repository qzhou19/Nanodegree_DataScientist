{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 Note: ETL & NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract: CSV, JSON, XML, SQL DB, API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper, open file to check first n lines\n",
    "def print_lines(n, file_name):\n",
    "    f = open(file_name)\n",
    "    for i in range(n):\n",
    "        print(f.readline())\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV\n",
    "import pandas as pd\n",
    "df = pd.read_csv('file', dtype=str, skiprows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON - method 1\n",
    "import pandas as pd\n",
    "df = pd.read_json('file', orient='records')\n",
    "# use orient option based on the file layouts\n",
    "\n",
    "# JSON - method 2\n",
    "import json\n",
    "with open('file') as f:\n",
    "    js_data = json.load(f)\n",
    "    \n",
    "# first record\n",
    "print(js_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML\n",
    "from bs4 import BeautifulSoup\n",
    "with open('file') as fp:\n",
    "    soup = BeautifulSoup(fp, \"lxml\") # lxml is the parser type\n",
    "\n",
    "# check first 5 records\n",
    "i = 0\n",
    "for recrod in soup.final_all('record'):\n",
    "    i += 1\n",
    "    for record in record.find_all('field'):\n",
    "        print(record['name'], ': ', record.text)\n",
    "    print()\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL - SQLite3\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "conn = sqlite3.connect('file.db')\n",
    "pd.read_sql('SELECT * FROM table', conn)\n",
    "\n",
    "# SQL - SQLAlchemy\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('sqlite:///../path/file.db')\n",
    "pd.read_sql('SELECT * FROM table', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = 'http://weblink'\n",
    "r = requests.get(url)\n",
    "r.json() # assume retrived file is json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('file.csv', index=False)\n",
    "df.to_json('file.json', orient='records')\n",
    "\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('file.db')\n",
    "df.to_sql('tablename', con = conn, if_exists='replace', index=False)\n",
    "\n",
    "# update database\n",
    "conn = sqlite3.connect('file.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute('DROP TABLE IF EXISTS tablename')\n",
    "cur.execute(\"CREATE TABLE tablename (col1 TEXT PRIMARY KEY, col2 REAL, col3 INTEGER);\")\n",
    "cur.execute(\"INSERT INTO tablename (col1, col2, col3) VALUES ('a', 1.2, 1);\")\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# select all\n",
    "cur.execute(\"SELECT * FROM tablename\")\n",
    "cur.fetchall()\n",
    "\n",
    "# helper: assign values from df to colnames\n",
    "col1, col2, col3 = df.iterrows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform: encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encodings.aliases import aliases\n",
    "\n",
    "# For file unclear what the encoding is\n",
    "for encoding in set(aliases.values()):\n",
    "    try:\n",
    "        df = pd.read_csv('file', encoding=encoding)\n",
    "        print('successful', encoding)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Or use chardet library to identify\n",
    "import chardet\n",
    "\n",
    "with open('file', 'rb') as file:\n",
    "    print(chardet.detect(file.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute by group\n",
    "df['col'] = df.sort_values('x').groupby('group')['col'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique value\n",
    "uni, count = np.unique(df, return_counts=True)\n",
    "df['col'],unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new cols systematically\n",
    "def create_newcol(b, k):\n",
    "# create new_col as 2b, 3b, .. until kb\n",
    "    new_col = []\n",
    "    for i in range(2, k+1):\n",
    "        new_col.append(b**i)\n",
    "    return new_col\n",
    "\n",
    "def create_newcol_name(colname, k):\n",
    "    col_names = []\n",
    "    for i in range(2, k+1):\n",
    "        col_names.append('{}{}'.format(colname, i))\n",
    "    return col_names\n",
    "\n",
    "def concate_newcol(df, column, k):\n",
    "    new_feature = df[column].apply(lambda x: create_newcol(x, k))\n",
    "    new_feature_df = pd.DataFrame(new_feature.tolist(), columns = create_newcol_name(column, k))\n",
    "    return pd.concat([df, new_feature_df], axis=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP\n",
    "#### 1. Cleaning\n",
    "#### 2. Normalizing\n",
    "#### 3. Tokenizing\n",
    "#### 4. Stop words\n",
    "#### 5. Parts of Speech POS tagging\n",
    "#### 6. Stemming and lemmatization\n",
    "#### 7. Bag of Words BOW and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# get text from a webpage\n",
    "r = requests.get(\"https://www.udacity.com/courses/all\")\n",
    "\n",
    "# remove HTML tags\n",
    "soup = BeautifulSoup(r.text)\n",
    "\n",
    "# find all summary - based on inspect the code of html\n",
    "summaries = soup.find_all(\"div\", {\"class\": \"course-summary-card\"})\n",
    "print(len(summaries)) # number of records\n",
    "print(summaries[0].prettify()) # checck first record\n",
    "\n",
    "# extract course title and school\n",
    "course = []\n",
    "for summ in summaries:\n",
    "    title = summ.select_one(\"h3\").get_text().strip()\n",
    "    school = summ.select_one(\"h4\").get_text().strip()\n",
    "    course.append((title, school))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the first time you see the second renaissance it may look boring  look at it at least twice and definitely watch part 2  it will change your view of the matrix  are the human people the ones who started the war   is ai a bad thing  '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalization\n",
    "# lower case\n",
    "text = text.lower()\n",
    "\n",
    "# remove punctuation\n",
    "import re\n",
    "\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'first', 'time', 'you', 'see', 'the', 'second', 'renaissance', 'it', 'may', 'look', 'boring', 'look', 'at', 'it', 'at', 'least', 'twice', 'and', 'definitely', 'watch', 'part', '2', 'it', 'will', 'change', 'your', 'view', 'of', 'the', 'matrix', 'are', 'the', 'human', 'people', 'the', 'ones', 'who', 'started', 'the', 'war', 'is', 'ai', 'a', 'bad', 'thing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\qz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "words = word_tokenize(text)\n",
    "sentences = sent_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'time', 'see', 'second', 'renaissance', 'may', 'look', 'boring', 'look', 'least', 'twice', 'definitely', 'watch', 'part', '2', 'change', 'view', 'matrix', 'human', 'people', 'ones', 'started', 'war', 'ai', 'bad', 'thing']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\qz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\qz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
     ]
    }
   ],
   "source": [
    "# POS tag\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pt = pos_tag(words)\n",
    "#print(pt)\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "tree = ne_chunk(pt)\n",
    "#print(tree)\n",
    "\n",
    "# Define a custom grammar\n",
    "my_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "parser = nltk.ChartParser(my_grammar)\n",
    "for tree in parser.parse(word_tokenize(\"I shot an elephant in my pajamas\")):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\qz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['first',\n",
       " 'time',\n",
       " 'see',\n",
       " 'second',\n",
       " 'renaissance',\n",
       " 'may',\n",
       " 'look',\n",
       " 'bore',\n",
       " 'look',\n",
       " 'least',\n",
       " 'twice',\n",
       " 'definitely',\n",
       " 'watch',\n",
       " 'part',\n",
       " '2',\n",
       " 'change',\n",
       " 'view',\n",
       " 'matrix',\n",
       " 'human',\n",
       " 'people',\n",
       " 'one',\n",
       " 'start',\n",
       " 'war',\n",
       " 'ai',\n",
       " 'bad',\n",
       " 'thing']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming and Lemmatizing\n",
    "import nltk\n",
    "nltk.download('wordnet') #lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "# print(stemmed)\n",
    "\n",
    "# lemmatization\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "#print(lemmed))\n",
    "\n",
    "# lemmatize verbs by specifying pos\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in lemmed]\n",
    "lemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\qz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\qz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\qz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words - CountVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "vect = CountVectorizer(tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 6,\n",
       " 'time': 20,\n",
       " 'see': 17,\n",
       " 'second': 16,\n",
       " 'renaissance': 15,\n",
       " 'may': 11,\n",
       " 'look': 9,\n",
       " 'boring': 3,\n",
       " 'least': 8,\n",
       " 'twice': 21,\n",
       " 'definitely': 5,\n",
       " 'watch': 24,\n",
       " 'part': 13,\n",
       " '2': 0,\n",
       " 'change': 4,\n",
       " 'view': 22,\n",
       " 'matrix': 10,\n",
       " 'human': 7,\n",
       " 'people': 14,\n",
       " 'one': 12,\n",
       " 'started': 18,\n",
       " 'war': 23,\n",
       " 'ai': 1,\n",
       " 'bad': 2,\n",
       " 'thing': 19}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"The first time you see The Second Renaissance it may look boring.\",\n",
    "        \"Look at it at least twice and definitely watch part 2.\",\n",
    "        \"It will change your view of the matrix.\",\n",
    "        \"Are the human people the ones who started the war?\",\n",
    "        \"Is AI a bad thing ?\"]\n",
    "\n",
    "x = vect.fit_transform(corpus).toarray()\n",
    "x\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.36419547, 0.        ,\n",
       "        0.        , 0.36419547, 0.        , 0.        , 0.26745392,\n",
       "        0.        , 0.36419547, 0.        , 0.        , 0.        ,\n",
       "        0.36419547, 0.36419547, 0.36419547, 0.        , 0.        ,\n",
       "        0.36419547, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.39105193, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.39105193, 0.        , 0.        , 0.39105193, 0.28717648,\n",
       "        0.        , 0.        , 0.        , 0.39105193, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.39105193, 0.        , 0.        , 0.39105193],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.57735027, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.4472136 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.4472136 , 0.        , 0.4472136 ,\n",
       "        0.        , 0.        , 0.        , 0.4472136 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.4472136 , 0.        ],\n",
       "       [0.        , 0.57735027, 0.57735027, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Transformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "tfidf = transformer.fit_transform(x).toarray()\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.30298183, 0.        , 0.        , 0.30298183, 0.        ,\n",
       "        0.        , 0.20291046, 0.        , 0.24444384, 0.        ,\n",
       "        0.30298183, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.30298183, 0.30298183, 0.30298183, 0.        , 0.40582093,\n",
       "        0.        , 0.30298183, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.30298183, 0.        ],\n",
       "       [0.        , 0.30015782, 0.        , 0.60031564, 0.        ,\n",
       "        0.        , 0.        , 0.30015782, 0.        , 0.        ,\n",
       "        0.        , 0.20101919, 0.30015782, 0.24216544, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.30015782, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.30015782, 0.        , 0.        ,\n",
       "        0.30015782, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.38077552, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.25500981, 0.        , 0.        , 0.38077552,\n",
       "        0.        , 0.38077552, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.25500981,\n",
       "        0.        , 0.        , 0.        , 0.38077552, 0.        ,\n",
       "        0.        , 0.        , 0.38077552, 0.        , 0.38077552],\n",
       "       [0.        , 0.        , 0.30101067, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.30101067,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.30101067, 0.        , 0.30101067,\n",
       "        0.        , 0.        , 0.        , 0.30101067, 0.60477106,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.30101067,\n",
       "        0.        , 0.30101067, 0.        , 0.        , 0.        ],\n",
       "       [0.5       , 0.        , 0.        , 0.        , 0.5       ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Vectorizer = TF-IDF Tranformer + Count Vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
